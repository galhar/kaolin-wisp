{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaolin'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapp_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m default_log_setup, args_to_log_format\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconfig_parser\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mconfig_parser\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WispState\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MultiviewDataset, SampleRays\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/config_parser.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dict, List, Any\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nefs\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m grids\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tracers\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/models/__init__.py:9\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# distribution of this software and related documentation without an express\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpipeline\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Pipeline\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/models/pipeline.py:10\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# distribution of this software and related documentation without an express\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnefs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseNeuralField\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtracers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_tracer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseTracer\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mPipeline\u001B[39;00m(nn\u001B[38;5;241m.\u001B[39mModule):\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/models/nefs/__init__.py:10\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# distribution of this software and related documentation without an express\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_nef\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mneural_sdf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnerf\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspc_field\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/models/nefs/neural_sdf.py:17\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mactivations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_activation_class\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecoders\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BasicDecoder\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgrids\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BLASGrid\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mNeuralSDF\u001B[39;00m(BaseNeuralField):\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;124;03m\"\"\"Model for encoding neural signed distance functions (implicit surfaces).\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;124;03m    This field implementation uses feature grids for faster and more efficient queries.\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124;03m    For example, the usage of Octree follows the idea from Takikawa et al. 2021 (Neural Geometric Level of Detail).\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/models/grids/__init__.py:9\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# distribution of this software and related documentation without an express\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mblas_grid\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moctree_grid\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcodebook_grid\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/models/grids/blas_grid.py:12\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dict, Any, Set, Type\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WispModule\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maccelstructs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseAS, ASQueryResults, ASRaytraceResults, ASRaymarchResults\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBLASGrid\u001B[39;00m(WispModule, ABC):\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m    BLASGrids (commonly referred in documentation as simply \"grids\"), represent feature grids in Wisp.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124;03m    BLAS: \"Bottom Level Acceleration Structure\", to signify this structure is the backbone that captures\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124;03m    possibly paired with decoders to form a neural field.\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/accelstructs/__init__.py:10\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# distribution of this software and related documentation without an express\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_as\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseAS, ASQueryResults, ASRaytraceResults, ASRaymarchResults\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moctree_as\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m OctreeAS\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maabb_as\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AxisAlignedBBoxAS\n",
      "File \u001B[0;32m~/Gits/kaolin-wisp/wisp/accelstructs/octree_as.py:12\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, Tuple\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkaolin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspc\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mspc_ops\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkaolin\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrender\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspc\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mspc_render\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwisp\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmesh\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmesh_ops\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'kaolin'"
     ]
    }
   ],
   "source": [
    "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION & AFFILIATES and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION & AFFILIATES is strictly prohibited.\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import wisp\n",
    "from wisp.app_utils import default_log_setup, args_to_log_format\n",
    "import wisp.config_parser as config_parser\n",
    "from wisp.framework import WispState\n",
    "from wisp.datasets import MultiviewDataset, SampleRays\n",
    "from wisp.models.grids import BLASGrid, OctreeGrid, CodebookOctreeGrid, TriplanarGrid, HashGrid\n",
    "from wisp.tracers import BaseTracer, PackedRFTracer\n",
    "from wisp.models.nefs import BaseNeuralField, NeuralRadianceField\n",
    "from wisp.models.pipeline import Pipeline\n",
    "from wisp.trainers import BaseTrainer, MultiviewTrainer\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Wisp mains define args per app.\n",
    "    Args are collected by priority: cli args > config yaml > argparse defaults\n",
    "    For convenience, args are divided into groups.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description='A script for training simple NeRF variants.',\n",
    "                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--config', type=str,\n",
    "                        help='Path to config file to replace defaults.')\n",
    "    parser.add_argument('--profile', action='store_true',\n",
    "                        help='Enable NVTX profiling')\n",
    "\n",
    "    log_group = parser.add_argument_group('logging')\n",
    "    log_group.add_argument('--exp-name', type=str,\n",
    "                           help='Experiment name, unique id for trainers, logs.')\n",
    "    log_group.add_argument('--log-level', action='store', type=int, default=logging.INFO,\n",
    "                           help='Logging level to use globally, DEBUG: 10, INFO: 20, WARN: 30, ERROR: 40.')\n",
    "    log_group.add_argument('--perf', action='store_true', default=False,\n",
    "                           help='Use high-level profiling for the trainer.')\n",
    "\n",
    "    data_group = parser.add_argument_group('dataset')\n",
    "    data_group.add_argument('--dataset-path', type=str,\n",
    "                            help='Path to the dataset')\n",
    "    data_group.add_argument('--dataset-num-workers', type=int, default=-1,\n",
    "                            help='Number of workers for dataset preprocessing, if it supports multiprocessing. '\n",
    "                                 '-1 indicates no multiprocessing.')\n",
    "    data_group.add_argument('--dataloader-num-workers', type=int, default=0,\n",
    "                            help='Number of workers for dataloader.')\n",
    "    data_group.add_argument('--bg-color', default='black' if is_interactive() else 'white',\n",
    "                            choices=['white', 'black'], help='Background color')\n",
    "    data_group.add_argument('--multiview-dataset-format', default='standard', choices=['standard', 'rtmv'],\n",
    "                            help='Data format for the transforms')\n",
    "    data_group.add_argument('--num-rays-sampled-per-img', type=int, default='4096',\n",
    "                            help='Number of rays to sample per image')\n",
    "    data_group.add_argument('--mip', type=int, default=None,\n",
    "                            help='MIP level of ground truth image')\n",
    "\n",
    "    grid_group = parser.add_argument_group('grid')\n",
    "    grid_group.add_argument('--grid-type', type=str, default='OctreeGrid',\n",
    "                            choices=config_parser.list_modules('grid'),\n",
    "                            help='Type of to use, i.e.:'\n",
    "                                 '\"OctreeGrid\", \"CodebookOctreeGrid\", \"TriplanarGrid\", \"HashGrid\".'\n",
    "                                 'Grids are located in `wisp.models.grids`')\n",
    "    grid_group.add_argument('--interpolation-type', type=str, default='linear', choices=['linear', 'closest'],\n",
    "                            help='Interpolation type to use for samples within grids.'\n",
    "                                 'For a 3D grid structure, linear uses trilinear interpolation of 8 cell nodes,'\n",
    "                                 'closest uses the nearest neighbor.')\n",
    "    grid_group.add_argument('--blas-type', type=str, default='octree',  # TODO(operel)\n",
    "                            choices=['octree', ],\n",
    "                            help='Type of acceleration structure to use for fast raymarch occupancy queries.')\n",
    "    grid_group.add_argument('--multiscale-type', type=str, default='sum', choices=['sum', 'cat'],\n",
    "                            help='Aggregation of choice for multi-level grids, for features from different LODs.')\n",
    "    grid_group.add_argument('--feature-dim', type=int, default=32,\n",
    "                            help='Dimensionality for features stored within the grid nodes.')\n",
    "    grid_group.add_argument('--feature-std', type=float, default=0.0,\n",
    "                            help='Grid initialization: standard deviation used for randomly sampling initial features.')\n",
    "    grid_group.add_argument('--feature-bias', type=float, default=0.0,\n",
    "                            help='Grid initialization: bias used for randomly sampling initial features.')\n",
    "    grid_group.add_argument('--base-lod', type=int, default=2,\n",
    "                            help='Number of levels in grid, which book-keep occupancy but not features.'\n",
    "                                 'The total number of levels in a grid is `base_lod + num_lod - 1`')\n",
    "    grid_group.add_argument('--num-lods', type=int, default=1,\n",
    "                            help='Number of levels in grid, which store concrete features.')\n",
    "    grid_group.add_argument('--codebook-bitwidth', type=int, default=8,\n",
    "                            help='For Codebook and HashGrids only: determines the table size as 2**(bitwidth).')\n",
    "    grid_group.add_argument('--tree-type', type=str, default='geometric', choices=['geometric', 'quad'],\n",
    "                            help='For HashGrids only: how the resolution of the grid is determined. '\n",
    "                                 '\"geometric\" uses the geometric sequence initialization from InstantNGP,'\n",
    "                                 'where \"quad\" uses an octree sampling pattern.')\n",
    "    grid_group.add_argument('--min-grid-res', type=int, default=16,\n",
    "                            help='For HashGrids only: min grid resolution, used only in geometric initialization mode')\n",
    "    grid_group.add_argument('--max-grid-res', type=int, default=2048,\n",
    "                            help='For HashGrids only: max grid resolution, used only in geometric initialization mode')\n",
    "    grid_group.add_argument('--prune-min-density', type=float, default=(0.01 * 512) / np.sqrt(3),\n",
    "                            help='For HashGrids only: Minimum density value for pruning')\n",
    "    grid_group.add_argument('--prune-density-decay', type=float, default=0.6,\n",
    "                            help='For HashGrids only: The decay applied on the density every pruning')\n",
    "    grid_group.add_argument('--blas-level', type=float, default=7,\n",
    "                            help='For HashGrids only: Determines the number of levels in the acceleration structure '\n",
    "                                 'used to track the occupancy status (bottom level acceleration structure).')\n",
    "\n",
    "    nef_group = parser.add_argument_group('nef')\n",
    "    nef_group.add_argument('--pos-embedder', type=str, choices=['none', 'identity', 'positional'],\n",
    "                           default='positional',\n",
    "                           help='MLP Decoder of neural field: Positional embedder used to encode input coordinates'\n",
    "                                'or view directions.')\n",
    "    nef_group.add_argument('--view-embedder', type=str, choices=['none', 'identity', 'positional'],\n",
    "                           default='positional',\n",
    "                           help='MLP Decoder of neural field: Positional embedder used to encode view direction')\n",
    "    nef_group.add_argument('--position-input', type=bool, default=False,\n",
    "                           help='If True, position coords will be concatenated to the '\n",
    "                                'features / positional embeddings when fed into the decoder.')\n",
    "    nef_group.add_argument('--pos-multires', type=int, default=10,\n",
    "                           help='MLP Decoder of neural field: Number of frequencies to use for positional encoding'\n",
    "                                'of input coordinates')\n",
    "    nef_group.add_argument('--view-multires', type=int, default=4,\n",
    "                           help='MLP Decoder of neural field: Number of frequencies to use for positional encoding'\n",
    "                                'of view direction')\n",
    "    nef_group.add_argument('--layer-type', type=str, default='none',\n",
    "                           choices=['none', 'spectral_norm', 'frobenius_norm', 'l_1_norm', 'l_inf_norm'])\n",
    "    nef_group.add_argument('--activation-type', type=str, default='relu',\n",
    "                           choices=['relu', 'sin'])\n",
    "    nef_group.add_argument('--hidden-dim', type=int, help='MLP Decoder of neural field: width of all hidden layers.')\n",
    "    nef_group.add_argument('--num-layers', type=int, help='MLP Decoder of neural field: number of hidden layers.')\n",
    "\n",
    "    tracer_group = parser.add_argument_group('tracer')\n",
    "    tracer_group.add_argument('--raymarch-type', type=str, choices=['ray', 'voxel'], default='ray',\n",
    "                              help='Marching algorithm to use when generating samples along rays in tracers.'\n",
    "                                   '`ray` samples fixed amount of randomized `num_steps` along the ray.'\n",
    "                                   '`voxel` samples `num_steps` samples in each cell the ray intersects.')\n",
    "    tracer_group.add_argument('--num-steps', type=int, default=1024,\n",
    "                              help='Number of samples to generate along traced rays. See --raymarch-type for '\n",
    "                                   'algorithm used to generate the samples.')\n",
    "\n",
    "    trainer_group = parser.add_argument_group('trainer')\n",
    "    trainer_group.add_argument('--epochs', type=int, default=250,\n",
    "                               help='Number of epochs to run the training.')\n",
    "    trainer_group.add_argument('--batch-size', type=int, default=512,\n",
    "                               help='Batch size for the training.')\n",
    "    trainer_group.add_argument('--resample', action='store_true',\n",
    "                               help='Resample the dataset after every epoch.')\n",
    "    trainer_group.add_argument('--only-last', action='store_true',\n",
    "                               help='Train only last LOD.')\n",
    "    trainer_group.add_argument('--resample-every', type=int, default=1,\n",
    "                               help='Resample every N epochs')\n",
    "    trainer_group.add_argument('--model-format', type=str, default='full', choices=['full', 'state_dict'],\n",
    "                               help='Format in which to save models.')\n",
    "    trainer_group.add_argument('--pretrained', type=str,\n",
    "                               help='Path to pretrained model weights.')\n",
    "    trainer_group.add_argument('--save-as-new', action='store_true',\n",
    "                               help='Save the model at every epoch (no overwrite).')\n",
    "    trainer_group.add_argument('--save-every', type=int, default=(-1 if is_interactive() else 5),\n",
    "                               help='Save the model at every N epoch.')\n",
    "    trainer_group.add_argument('--render-tb-every', type=int, default=(-1 if is_interactive() else 5),\n",
    "                               help='Render every N epochs')\n",
    "    trainer_group.add_argument('--log-tb-every', type=int, default=5,  # TODO (operel): move to logging\n",
    "                               help='Render to tensorboard every N epochs')\n",
    "    trainer_group.add_argument('--log-dir', type=str, default='_results/logs/runs/',\n",
    "                               help='Log file directory for checkpoints.')\n",
    "    trainer_group.add_argument('--prune-every', type=int, default=-1,\n",
    "                               help='Prune every N epochs')\n",
    "    trainer_group.add_argument('--grow-every', type=int, default=-1,\n",
    "                               help='Grow network every X epochs')\n",
    "    trainer_group.add_argument('--growth-strategy', type=str, default='increase',\n",
    "                               choices=['onebyone',  # One by one trains one level at a time.\n",
    "                                        'increase',  # Increase starts from [0] and ends up at [0,...,N]\n",
    "                                        'shrink',  # Shrink strats from [0,...,N] and ends up at [N]\n",
    "                                        'finetocoarse',  # Fine to coarse starts from [N] and ends up at [0,...,N]\n",
    "                                        'onlylast'],  # Only last starts and ends at [N]\n",
    "                               help='Strategy for coarse-to-fine training')\n",
    "    trainer_group.add_argument('--valid-only', action='store_true',\n",
    "                               help='Run validation only (and do not run training).')\n",
    "    trainer_group.add_argument('--valid-every', type=int, default=-1,\n",
    "                               help='Frequency of running validation.')\n",
    "    trainer_group.add_argument('--random-lod', action='store_true',\n",
    "                               help='Use random lods to train.')\n",
    "    trainer_group.add_argument('--wandb-project', type=str, default=None,\n",
    "                               help='Weights & Biases Project')\n",
    "    trainer_group.add_argument('--wandb-run-name', type=str, default=None,\n",
    "                               help='Weights & Biases Run Name')\n",
    "    trainer_group.add_argument('--wandb-entity', type=str, default=None,\n",
    "                               help='Weights & Biases Entity')\n",
    "    trainer_group.add_argument('--wandb-viz-nerf-angles', type=int, default=20,\n",
    "                               help='Number of Angles to visualize a scene on Weights & Biases. '\n",
    "                                    'Set this to 0 to disable 360 degree visualizations.')\n",
    "    trainer_group.add_argument('--wandb-viz-nerf-distance', type=int, default=3,\n",
    "                               help='Distance to visualize Scene from on Weights & Biases')\n",
    "\n",
    "    optimizer_group = parser.add_argument_group('optimizer')\n",
    "    optimizer_group.add_argument('--optimizer-type', type=str, default='adam',\n",
    "                                 choices=config_parser.list_modules('optim'),\n",
    "                                 help='Optimizer to be used, includes optimizer modules available within `torch.optim` '\n",
    "                                      'and fused optimizers from `apex`, if apex is installed.')\n",
    "    optimizer_group.add_argument('--lr', type=float, default=0.001,\n",
    "                                 help='Base optimizer learning rate.')\n",
    "    optimizer_group.add_argument('--eps', type=float, default=1e-8,\n",
    "                                 help='Eps value for numerical stability.')\n",
    "    optimizer_group.add_argument('--weight-decay', type=float, default=0,\n",
    "                                 help='Weight decay, applied only to decoder weights.')\n",
    "    optimizer_group.add_argument('--grid-lr-weight', type=float, default=100.0,\n",
    "                                 help='Relative learning rate weighting applied only for the grid parameters'\n",
    "                                      '(e.g. parameters which contain \"grid\" in their name)')\n",
    "    optimizer_group.add_argument('--rgb-loss', type=float, default=1.0,\n",
    "                                 help='Weight of rgb loss')\n",
    "\n",
    "    # Evaluation renderer (definitions do not affect interactive renderer)\n",
    "    offline_renderer_group = parser.add_argument_group('renderer')\n",
    "    offline_renderer_group.add_argument('--render-res', type=int, nargs=2, default=[512, 512],\n",
    "                                        help='Width/height to render at.')\n",
    "    offline_renderer_group.add_argument('--render-batch', type=int, default=0,\n",
    "                                        help='Batch size (in number of rays) for batched rendering.')\n",
    "    offline_renderer_group.add_argument('--camera-origin', type=float, nargs=3, default=[-2.8, 2.8, -2.8],\n",
    "                                        help='Camera origin.')\n",
    "    offline_renderer_group.add_argument('--camera-lookat', type=float, nargs=3, default=[0, 0, 0],\n",
    "                                        help='Camera look-at/target point.')\n",
    "    offline_renderer_group.add_argument('--camera-fov', type=float, default=30,\n",
    "                                        help='Camera field of view (FOV).')\n",
    "    offline_renderer_group.add_argument('--camera-proj', type=str, choices=['ortho', 'persp'], default='persp',\n",
    "                                        help='Camera projection.')\n",
    "    offline_renderer_group.add_argument('--camera-clamp', nargs=2, type=float, default=[0, 10],\n",
    "                                        help='Camera clipping bounds.')\n",
    "\n",
    "    # Parse CLI args & config files\n",
    "    args = config_parser.parse_args(parser)\n",
    "\n",
    "    # Also obtain args as grouped hierarchy, useful for, i.e., logging\n",
    "    args_dict = config_parser.get_grouped_args(parser, args)\n",
    "    return args, args_dict\n",
    "\n",
    "\n",
    "def load_dataset(args) -> MultiviewDataset:\n",
    "    \"\"\" Loads a multiview dataset comprising of pairs of images and calibrated cameras.\n",
    "    The types of supported datasets are defined by multiview_dataset_format:\n",
    "    'standard' - refers to the standard NeRF format popularized by Mildenhall et al. 2020,\n",
    "                 including additions to the metadata format added by Muller et al. 2022.\n",
    "    'rtmv' - refers to the dataset published by Tremblay et. al 2022,\n",
    "            \"RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis\".\n",
    "            This dataset includes depth information which allows for performance improving optimizations in some cases.\n",
    "    \"\"\"\n",
    "    transform = SampleRays(num_samples=args.num_rays_sampled_per_img)\n",
    "    train_dataset = wisp.datasets.load_multiview_dataset(dataset_path=args.dataset_path,\n",
    "                                                         split='train',\n",
    "                                                         mip=args.mip,\n",
    "                                                         bg_color=args.bg_color,\n",
    "                                                         dataset_num_workers=args.dataset_num_workers,\n",
    "                                                         transform=transform)\n",
    "    validation_dataset = None\n",
    "    if args.valid_every > -1 or args.valid_only:\n",
    "        validation_dataset = train_dataset.create_split(split='val', transform=None)\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "def load_grid(args, dataset: MultiviewDataset) -> BLASGrid:\n",
    "    \"\"\" Wisp's implementation of NeRF uses feature grids to improve the performance and quality (allowing therefore,\n",
    "    interactivity).\n",
    "    This function loads the feature grid to use within the neural pipeline.\n",
    "    Grid choices are interesting to explore, so we leave the exact backbone type configurable,\n",
    "    and show how grid instances may be explicitly constructed.\n",
    "    Grids choices, for example, are: OctreeGrid, TriplanarGrid, HashGrid, CodebookOctreeGrid\n",
    "    See corresponding grid constructors for each of their arg details.\n",
    "    \"\"\"\n",
    "    grid = None\n",
    "\n",
    "    # Optimization: For octrees based grids, if dataset contains depth info, initialize only cells known to be occupied\n",
    "    if args.grid_type == \"OctreeGrid\":\n",
    "        if dataset.supports_depth():\n",
    "            grid = OctreeGrid.from_pointcloud(\n",
    "                pointcloud=dataset.as_pointcloud(),\n",
    "                feature_dim=args.feature_dim,\n",
    "                base_lod=args.base_lod,\n",
    "                num_lods=args.num_lods,\n",
    "                interpolation_type=args.interpolation_type,\n",
    "                multiscale_type=args.multiscale_type,\n",
    "                feature_std=args.feature_std,\n",
    "                feature_bias=args.feature_bias,\n",
    "            )\n",
    "        else:\n",
    "            grid = OctreeGrid.make_dense(\n",
    "                feature_dim=args.feature_dim,\n",
    "                base_lod=args.base_lod,\n",
    "                num_lods=args.num_lods,\n",
    "                interpolation_type=args.interpolation_type,\n",
    "                multiscale_type=args.multiscale_type,\n",
    "                feature_std=args.feature_std,\n",
    "                feature_bias=args.feature_bias,\n",
    "            )\n",
    "    elif args.grid_type == \"CodebookOctreeGrid\":\n",
    "        if dataset.supports_depth():\n",
    "            grid = CodebookOctreeGrid.from_pointcloud(\n",
    "                pointcloud=dataset.as_pointcloud(),\n",
    "                feature_dim=args.feature_dim,\n",
    "                base_lod=args.base_lod,\n",
    "                num_lods=args.num_lods,\n",
    "                interpolation_type=args.interpolation_type,\n",
    "                multiscale_type=args.multiscale_type,\n",
    "                feature_std=args.feature_std,\n",
    "                feature_bias=args.feature_bias,\n",
    "                codebook_bitwidth=args.codebook_bitwidth\n",
    "            )\n",
    "        else:\n",
    "            grid = CodebookOctreeGrid.make_dense(\n",
    "                feature_dim=args.feature_dim,\n",
    "                base_lod=args.base_lod,\n",
    "                num_lods=args.num_lods,\n",
    "                interpolation_type=args.interpolation_type,\n",
    "                multiscale_type=args.multiscale_type,\n",
    "                feature_std=args.feature_std,\n",
    "                feature_bias=args.feature_bias,\n",
    "                codebook_bitwidth=args.codebook_bitwidth\n",
    "            )\n",
    "    elif args.grid_type == \"TriplanarGrid\":\n",
    "        grid = TriplanarGrid(\n",
    "            feature_dim=args.feature_dim,\n",
    "            base_lod=args.base_lod,\n",
    "            num_lods=args.num_lods,\n",
    "            interpolation_type=args.interpolation_type,\n",
    "            multiscale_type=args.multiscale_type,\n",
    "            feature_std=args.feature_std,\n",
    "            feature_bias=args.feature_bias,\n",
    "        )\n",
    "    elif args.grid_type == \"HashGrid\":\n",
    "        # \"geometric\" - determines the resolution of the grid using geometric sequence initialization from InstantNGP,\n",
    "        if args.tree_type == \"geometric\":\n",
    "            grid = HashGrid.from_geometric(\n",
    "                feature_dim=args.feature_dim,\n",
    "                num_lods=args.num_lods,\n",
    "                multiscale_type=args.multiscale_type,\n",
    "                feature_std=args.feature_std,\n",
    "                feature_bias=args.feature_bias,\n",
    "                codebook_bitwidth=args.codebook_bitwidth,\n",
    "                min_grid_res=args.min_grid_res,\n",
    "                max_grid_res=args.max_grid_res,\n",
    "                blas_level=args.blas_level\n",
    "            )\n",
    "        # \"quad\" - determines the resolution of the grid using an octree sampling pattern.\n",
    "        elif args.tree_type == \"octree\":\n",
    "            grid = HashGrid.from_octree(\n",
    "                feature_dim=args.feature_dim,\n",
    "                base_lod=args.base_lod,\n",
    "                num_lods=args.num_lods,\n",
    "                multiscale_type=args.multiscale_type,\n",
    "                feature_std=args.feature_std,\n",
    "                feature_bias=args.feature_bias,\n",
    "                codebook_bitwidth=args.codebook_bitwidth,\n",
    "                blas_level=args.blas_level\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown grid_type argument: {args.grid_type}\")\n",
    "    return grid\n",
    "\n",
    "\n",
    "def load_neural_field(args, dataset: MultiviewDataset) -> BaseNeuralField:\n",
    "    \"\"\" Creates a \"Neural Field\" instance which converts input coordinates to some output signal.\n",
    "    Here a NeuralRadianceField is created, which maps 3D coordinates (+ 2D view direction) -> RGB + density.\n",
    "    The NeuralRadianceField uses spatial feature grids internally for faster feature interpolation and raymarching.\n",
    "    \"\"\"\n",
    "    grid = load_grid(args=args, dataset=dataset)\n",
    "    nef = NeuralRadianceField(\n",
    "        grid=grid,\n",
    "        pos_embedder=args.pos_embedder,\n",
    "        view_embedder=args.view_embedder,\n",
    "        position_input=args.position_input,\n",
    "        pos_multires=args.pos_multires,\n",
    "        view_multires=args.view_multires,\n",
    "        activation_type=args.activation_type,\n",
    "        layer_type=args.layer_type,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        num_layers=args.num_layers,\n",
    "        prune_density_decay=args.prune_density_decay,  # Used only for grid types which support pruning\n",
    "        prune_min_density=args.prune_min_density  # Used only for grid types which support pruning\n",
    "    )\n",
    "    return nef\n",
    "\n",
    "\n",
    "def load_tracer(args) -> BaseTracer:\n",
    "    \"\"\" Wisp \"Tracers\" are responsible for taking input rays, marching them through the neural field to render\n",
    "    an output RenderBuffer.\n",
    "    Wisp's implementation of NeRF uses the PackedRFTracer to trace the neural field:\n",
    "    - Packed: each ray yields a custom number of samples, which are therefore packed in a flat form within a tensor,\n",
    "     see: https://kaolin.readthedocs.io/en/latest/modules/kaolin.ops.batch.html#packed\n",
    "    - RF: Radiance Field\n",
    "    PackedRFTracer is employed within the training loop, and is responsible for making use of the neural field's\n",
    "    grid to generate samples and decode them to pixel values.\n",
    "    \"\"\"\n",
    "    tracer = PackedRFTracer(\n",
    "        raymarch_type=args.raymarch_type,  # Chooses the ray-marching algorithm\n",
    "        num_steps=args.num_steps,  # Number of steps depends on raymarch_type\n",
    "        bg_color=args.bg_color\n",
    "    )\n",
    "    return tracer\n",
    "\n",
    "\n",
    "def load_neural_pipeline(args, dataset, device) -> Pipeline:\n",
    "    \"\"\" In Wisp, a Pipeline comprises of a neural field + a tracer (the latter is optional in some cases).\n",
    "    Together, they form the complete pipeline required to render a neural primitive from input rays / coordinates.\n",
    "    \"\"\"\n",
    "    nef = load_neural_field(args=args, dataset=dataset)\n",
    "    tracer = load_tracer(args=args)\n",
    "    pipeline = Pipeline(nef=nef, tracer=tracer)\n",
    "    if args.pretrained:\n",
    "        if args.model_format == \"full\":\n",
    "            pipeline = torch.load(args.pretrained)\n",
    "        else:\n",
    "            pipeline.load_state_dict(torch.load(args.pretrained))\n",
    "    pipeline.to(device)\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def load_trainer(pipeline, train_dataset, validation_dataset, device, scene_state, args, args_dict) -> BaseTrainer:\n",
    "    \"\"\" Loads the NeRF trainer.\n",
    "    The trainer is responsible for managing the optimization life-cycles and can be operated in 2 modes:\n",
    "    - Headless, which will run the train() function until all training steps are exhausted.\n",
    "    - Interactive mode, which uses the gui. In this case, an OptimizationApp uses events to prompt the trainer to\n",
    "      take training steps, while also taking care to render output to users (see: iterate()).\n",
    "      In interactive mode, trainers can also share information with the app through the scene_state (WispState object).\n",
    "    \"\"\"\n",
    "    # args.optimizer_type is the name of some optimizer class (from torch.optim or apex),\n",
    "    # Wisp's config_parser is able to pick this app's args with corresponding names to the optimizer constructor args.\n",
    "    # The actual construction of the optimizer instance happens within the trainer.\n",
    "    optimizer_cls = config_parser.get_module(name=args.optimizer_type)\n",
    "    optimizer_params = config_parser.get_args_for_function(args, optimizer_cls)\n",
    "\n",
    "    trainer = MultiviewTrainer(pipeline=pipeline,\n",
    "                               train_dataset=train_dataset,\n",
    "                               validation_dataset=validation_dataset,\n",
    "                               num_epochs=args.epochs,\n",
    "                               batch_size=args.batch_size,\n",
    "                               optim_cls=optimizer_cls,\n",
    "                               lr=args.lr,\n",
    "                               weight_decay=args.weight_decay,\n",
    "                               grid_lr_weight=args.grid_lr_weight,\n",
    "                               optim_params=optimizer_params,\n",
    "                               log_dir=args.log_dir,\n",
    "                               device=device,\n",
    "                               exp_name=args.exp_name,\n",
    "                               info=args_to_log_format(args_dict),\n",
    "                               extra_args=vars(args),\n",
    "                               render_tb_every=args.render_tb_every,\n",
    "                               save_every=args.save_every,\n",
    "                               scene_state=scene_state,\n",
    "                               trainer_mode='validate' if args.valid_only else 'train',\n",
    "                               using_wandb=args.wandb_project is not None)\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def load_app(args, scene_state, trainer):\n",
    "    \"\"\" Used only in interactive mode. Creates an interactive app, which employs a renderer which displays\n",
    "    the latest information from the trainer (see: OptimizationApp).\n",
    "    The OptimizationApp can be customized or further extend to support even more functionality.\n",
    "    \"\"\"\n",
    "    if not is_interactive():\n",
    "        logging.info(\"Running headless. For the app, set $WISP_HEADLESS=0.\")\n",
    "        return None  # Interactive mode is disabled\n",
    "    else:\n",
    "        from wisp.renderer.app.optimization_app import OptimizationApp\n",
    "        scene_state.renderer.device = trainer.device  # Use same device for trainer and app renderer\n",
    "        app = OptimizationApp(wisp_state=scene_state,\n",
    "                              trainer_step_func=trainer.iterate,\n",
    "                              experiment_name=\"wisp trainer\")\n",
    "        return app\n",
    "\n",
    "\n",
    "def is_interactive() -> bool:\n",
    "    \"\"\" Returns True if interactive mode with gui is on, False is HEADLESS mode is forced \"\"\"\n",
    "    return os.environ.get('WISP_HEADLESS') != '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "insert_args_to_cli = [\n",
    "    '--dataset-path /home/galharari/datasets/nerf_llff_data/fern_try_colmap/',\n",
    "    '--config app/nerf/configs/nerf_hash.yaml',\n",
    "    '--wandb-project wisp_playing',\n",
    "    '--wandb-viz-nerf-distance 2',\n",
    "    '--pretrained _results/logs/runs/test-nerf/20230209-232641/model.pth'\n",
    "]\n",
    "for arg_to_add in insert_args_to_cli:\n",
    "    if arg_to_add.split(' ')[0] in sys.argv:\n",
    "        continue\n",
    "    sys.argv.append(arg_to_add)\n",
    "args, args_dict = parse_args()  # Obtain args by priority: cli args > config yaml > argparse defaults\n",
    "default_log_setup(args.log_level)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load Everything\n",
    "train_dataset, validation_dataset = load_dataset(args=args)\n",
    "pipeline = load_neural_pipeline(args=args, dataset=train_dataset, device=device)\n",
    "scene_state = WispState()  # Joint trainer / app state\n",
    "trainer = load_trainer(pipeline=pipeline,\n",
    "                       train_dataset=train_dataset, validation_dataset=validation_dataset,\n",
    "                       device=device, scene_state=scene_state,\n",
    "                       args=args, args_dict=args_dict)\n",
    "\n",
    "# Now render\n",
    "trainer.pipeline.eval()\n",
    "logging.info(\"Beginning rendering...\")\n",
    "img_shape = trainer.train_dataset.img_shape\n",
    "logging.info(f\"Running rendering on dataset with {len(trainer.train_dataset)} images \"\n",
    "         f\"at resolution {img_shape[0]}x{img_shape[1]}\")\n",
    "\n",
    "trainer.valid_log_dir = os.path.join(trainer.log_dir, \"lego_5_views_train_feature_2d_renders\")\n",
    "logging.info(f\"Saving rendering result to {trainer.valid_log_dir}\")\n",
    "if not os.path.exists(trainer.valid_log_dir):\n",
    "    os.makedirs(trainer.valid_log_dir)\n",
    "\n",
    "lods = list(range(trainer.pipeline.nef.grid.num_lods))\n",
    "\n",
    "pips_model = None\n",
    "dataset = trainer.train_dataset\n",
    "lod_idx = lods[-1]\n",
    "name = f\"lod{lods[-1]}\"\n",
    "\n",
    "img_count = len(dataset)\n",
    "img_shape = dataset.img_shape\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
